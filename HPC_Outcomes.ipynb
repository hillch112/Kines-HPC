{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48721b68-3ce7-403f-8b3f-05842ffb679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /Users/006490246/anaconda3/lib/python3.11/site-packages (4.6.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/006490246/anaconda3/lib/python3.11/site-packages (from optuna) (1.18.0)\n",
      "Requirement already satisfied: colorlog in /Users/006490246/anaconda3/lib/python3.11/site-packages (from optuna) (6.10.1)\n",
      "Requirement already satisfied: numpy in /Users/006490246/anaconda3/lib/python3.11/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/006490246/anaconda3/lib/python3.11/site-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/006490246/anaconda3/lib/python3.11/site-packages (from optuna) (2.0.30)\n",
      "Requirement already satisfied: tqdm in /Users/006490246/anaconda3/lib/python3.11/site-packages (from optuna) (4.66.4)\n",
      "Requirement already satisfied: PyYAML in /Users/006490246/anaconda3/lib/python3.11/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in /Users/006490246/anaconda3/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /Users/006490246/anaconda3/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/006490246/anaconda3/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24dd0121-e0f3-411a-8d01-b2627aa24d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9472804b-b17b-46bf-818b-4b6d33368b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Optuna settings\n",
    "N_TRIALS        = 30          # change as needed on HPC\n",
    "N_EPOCHS_TUNE   = 12          # short runs for tuning\n",
    "EPOCHS_FINAL    = 25          # longer run for final training\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0135fa3e-ccef-4e6c-98b0-0970703cd912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SECTION 1: LOADING DATA FROM CSV\n",
      "Looking for: /Users/006490246/Desktop/statcast_4years.csv\n",
      "Exists? True\n",
      "Loaded data shape: (3080411, 118)\n",
      "Columns: ['pitch_type', 'game_date', 'release_speed', 'release_pos_x', 'release_pos_z', 'player_name', 'batter', 'pitcher', 'events', 'description', 'spin_dir', 'spin_rate_deprecated', 'break_angle_deprecated', 'break_length_deprecated', 'zone', 'des', 'game_type', 'stand', 'p_throws', 'home_team', 'away_team', 'type', 'hit_location', 'bb_type', 'balls', 'strikes', 'game_year', 'pfx_x', 'pfx_z', 'plate_x', 'plate_z', 'on_3b', 'on_2b', 'on_1b', 'outs_when_up', 'inning', 'inning_topbot', 'hc_x', 'hc_y', 'tfs_deprecated', 'tfs_zulu_deprecated', 'umpire', 'sv_id', 'vx0', 'vy0', 'vz0', 'ax', 'ay', 'az', 'sz_top', 'sz_bot', 'hit_distance_sc', 'launch_speed', 'launch_angle', 'effective_speed', 'release_spin_rate', 'release_extension', 'game_pk', 'fielder_2', 'fielder_3', 'fielder_4', 'fielder_5', 'fielder_6', 'fielder_7', 'fielder_8', 'fielder_9', 'release_pos_y', 'estimated_ba_using_speedangle', 'estimated_woba_using_speedangle', 'woba_value', 'woba_denom', 'babip_value', 'iso_value', 'launch_speed_angle', 'at_bat_number', 'pitch_number', 'pitch_name', 'home_score', 'away_score', 'bat_score', 'fld_score', 'post_away_score', 'post_home_score', 'post_bat_score', 'post_fld_score', 'if_fielding_alignment', 'of_fielding_alignment', 'spin_axis', 'delta_home_win_exp', 'delta_run_exp', 'bat_speed', 'swing_length', 'estimated_slg_using_speedangle', 'delta_pitcher_run_exp', 'hyper_speed', 'home_score_diff', 'bat_score_diff', 'home_win_exp', 'bat_win_exp', 'age_pit_legacy', 'age_bat_legacy', 'age_pit', 'age_bat', 'n_thruorder_pitcher', 'n_priorpa_thisgame_player_at_bat', 'pitcher_days_since_prev_game', 'batter_days_since_prev_game', 'pitcher_days_until_next_game', 'batter_days_until_next_game', 'api_break_z_with_gravity', 'api_break_x_arm', 'api_break_x_batter_in', 'arm_angle', 'attack_angle', 'attack_direction', 'swing_path_tilt', 'intercept_ball_minus_batter_pos_x_inches', 'intercept_ball_minus_batter_pos_y_inches']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSECTION 1: LOADING DATA FROM CSV\")\n",
    "csv_path = \"/Users/006490246/Desktop/statcast_4years.csv\"\n",
    "print(\"Looking for:\", csv_path)\n",
    "print(\"Exists?\", os.path.exists(csv_path))\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "print(f\"Loaded data shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "212c4f1a-ca9e-4fc1-a483-36e0e89e8cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SECTION 2: FILTERING TO BALLS IN PLAY (HITS + OUTS)\n",
      "\n",
      "✓ Filtered to balls in play that are hits or outs\n",
      "  Rows: 537,681\n",
      "\n",
      "Event distribution:\n",
      "events\n",
      "field_out                    320871\n",
      "single                       113984\n",
      "double                        34962\n",
      "home_run                      24383\n",
      "force_out                     15587\n",
      "grounded_into_double_play     14496\n",
      "sac_fly                        5424\n",
      "triple                         2973\n",
      "sac_bunt                       1933\n",
      "double_play                    1673\n",
      "fielders_choice_out            1300\n",
      "sac_fly_double_play              82\n",
      "triple_play                      12\n",
      "sac_bunt_double_play              1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSECTION 2: FILTERING TO BALLS IN PLAY (HITS + OUTS)\")\n",
    "\n",
    "# Define hit and out events\n",
    "hit_types = [\"single\", \"double\", \"triple\", \"home_run\"]\n",
    "\n",
    "out_event_types = [\n",
    "    \"field_out\",\n",
    "    \"force_out\",\n",
    "    \"double_play\",\n",
    "    \"triple_play\",\n",
    "    \"grounded_into_double_play\",\n",
    "    \"other_out\",\n",
    "    \"sac_fly\",\n",
    "    \"sac_fly_double_play\",\n",
    "    \"sac_bunt\",\n",
    "    \"sac_bunt_double_play\",\n",
    "    \"fielders_choice_out\",\n",
    "]\n",
    "\n",
    "# Balls in play: type == 'X'\n",
    "bip_mask = df[\"type\"] == \"X\"\n",
    "event_mask = df[\"events\"].isin(hit_types + out_event_types)\n",
    "\n",
    "data = df[bip_mask & event_mask].copy()\n",
    "\n",
    "print(f\"\\n✓ Filtered to balls in play that are hits or outs\")\n",
    "print(f\"  Rows: {len(data):,}\")\n",
    "print(\"\\nEvent distribution:\")\n",
    "print(data[\"events\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35a334b9-f6b7-4574-aa86-72c1b744f90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SECTION 3: BUILDING 5-OUTCOME TARGET\n",
      "\n",
      "✓ Created 5 outcome columns:\n",
      "   outs_in_play  single  double  triple  home_run\n",
      "0             1       0       0       0         0\n",
      "2             0       1       0       0         0\n",
      "3             0       1       0       0         0\n",
      "4             1       0       0       0         0\n",
      "5             0       0       1       0         0\n",
      "\n",
      "Outcome counts:\n",
      "outs_in_play    361379\n",
      "single          113984\n",
      "double           34962\n",
      "triple            2973\n",
      "home_run         24383\n",
      "dtype: int64\n",
      "\n",
      "Outcome proportions (%):\n",
      "outs_in_play    67.21\n",
      "single          21.20\n",
      "double           6.50\n",
      "triple           0.55\n",
      "home_run         4.53\n",
      "dtype: float64\n",
      "\n",
      "Outcome_class distribution (0=out,1=1B,2=2B,3=3B,4=HR):\n",
      "outcome_class\n",
      "0    361379\n",
      "1    113984\n",
      "2     34962\n",
      "3      2973\n",
      "4     24383\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSECTION 3: BUILDING 5-OUTCOME TARGET\")\n",
    "\n",
    "# 5 outcome columns (one-hot style, mutually exclusive)\n",
    "data[\"outs_in_play\"] = data[\"events\"].isin(out_event_types).astype(int)\n",
    "data[\"single\"]       = (data[\"events\"] == \"single\").astype(int)\n",
    "data[\"double\"]       = (data[\"events\"] == \"double\").astype(int)\n",
    "data[\"triple\"]       = (data[\"events\"] == \"triple\").astype(int)\n",
    "data[\"home_run\"]     = (data[\"events\"] == \"home_run\").astype(int)\n",
    "\n",
    "outcome_cols = [\"outs_in_play\", \"single\", \"double\", \"triple\", \"home_run\"]\n",
    "\n",
    "# Sanity check\n",
    "row_sum = data[outcome_cols].sum(axis=1)\n",
    "assert (row_sum == 1).all(), \"Some rows do not map to exactly one of the 5 outcomes!\"\n",
    "\n",
    "print(\"\\n✓ Created 5 outcome columns:\")\n",
    "print(data[outcome_cols].head())\n",
    "print(\"\\nOutcome counts:\")\n",
    "print(data[outcome_cols].sum())\n",
    "print(\"\\nOutcome proportions (%):\")\n",
    "print((data[outcome_cols].mean() * 100).round(2))\n",
    "\n",
    "# Build a single 5-class label: 0–4\n",
    "# 0 = outs_in_play, 1 = single, 2 = double, 3 = triple, 4 = home_run\n",
    "conditions = [\n",
    "    data[\"outs_in_play\"] == 1,\n",
    "    data[\"single\"] == 1,\n",
    "    data[\"double\"] == 1,\n",
    "    data[\"triple\"] == 1,\n",
    "    data[\"home_run\"] == 1,\n",
    "]\n",
    "choices = [0, 1, 2, 3, 4]\n",
    "data[\"outcome_class\"] = np.select(conditions, choices, default=-1).astype(int)\n",
    "assert (data[\"outcome_class\"] >= 0).all(), \"Found rows with invalid outcome_class!\"\n",
    "\n",
    "print(\"\\nOutcome_class distribution (0=out,1=1B,2=2B,3=3B,4=HR):\")\n",
    "print(data[\"outcome_class\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aafcbfa-78c3-4097-8afe-55e17e2f8587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SECTION 4: CREATING NEW FEATURES\n",
      "\n",
      "Creating derived features...\n",
      "  ✓ spray_angle: Direction of batted ball\n",
      "  ✓ horizontal_distance: |hc_x - center|\n",
      "  ✓ pitch_distance_from_center: distance from zone center\n",
      "  ✓ count: 'balls-strikes' representation\n",
      "  ✓ runners_on: total base runners\n",
      "  ✓ launch_speed_x_angle: launch_speed * launch_angle\n",
      "\n",
      "✓ Feature engineering complete\n",
      "  Total columns now: 130\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSECTION 4: CREATING NEW FEATURES\")\n",
    "\n",
    "print(\"\\nCreating derived features...\")\n",
    "\n",
    "# spray_angle: direction of batted ball using home plate reference (125.42, 125.42)\n",
    "if \"hc_x\" in data.columns and \"hc_y\" in data.columns:\n",
    "    data[\"spray_angle\"] = (\n",
    "        np.arctan2(data[\"hc_y\"] - 125.42, data[\"hc_x\"] - 125.42) * 180 / np.pi\n",
    "    )\n",
    "    print(\"  ✓ spray_angle: Direction of batted ball\")\n",
    "\n",
    "# horizontal_distance: how far left/right the ball went\n",
    "if \"hc_x\" in data.columns:\n",
    "    data[\"horizontal_distance\"] = np.abs(data[\"hc_x\"] - 125.42)\n",
    "    print(\"  ✓ horizontal_distance: |hc_x - center|\")\n",
    "\n",
    "# pitch_distance_from_center: distance from middle of the strike zone\n",
    "if \"plate_x\" in data.columns and \"plate_z\" in data.columns:\n",
    "    data[\"pitch_distance_from_center\"] = np.sqrt(\n",
    "        data[\"plate_x\"] ** 2 + (data[\"plate_z\"] - 2.5) ** 2\n",
    "    )\n",
    "    print(\"  ✓ pitch_distance_from_center: distance from zone center\")\n",
    "\n",
    "# count: ball-strike count (categorical)\n",
    "if \"balls\" in data.columns and \"strikes\" in data.columns:\n",
    "    data[\"count\"] = data[\"balls\"].astype(str) + \"-\" + data[\"strikes\"].astype(str)\n",
    "    print(\"  ✓ count: 'balls-strikes' representation\")\n",
    "\n",
    "# runners_on: total number of baserunners\n",
    "runner_columns = [\"on_1b\", \"on_2b\", \"on_3b\"]\n",
    "if all(col in data.columns for col in runner_columns):\n",
    "    data[\"runners_on\"] = (~data[runner_columns].isna()).sum(axis=1)\n",
    "    print(\"  ✓ runners_on: total base runners\")\n",
    "\n",
    "# launch_speed_x_angle: interaction between EV and LA\n",
    "if \"launch_speed\" in data.columns and \"launch_angle\" in data.columns:\n",
    "    data[\"launch_speed_x_angle\"] = data[\"launch_speed\"] * data[\"launch_angle\"]\n",
    "    print(\"  ✓ launch_speed_x_angle: launch_speed * launch_angle\")\n",
    "\n",
    "print(f\"\\n✓ Feature engineering complete\")\n",
    "print(f\"  Total columns now: {len(data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e110c0bf-8115-443f-b747-9201d8817020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SECTION 5: SELECTING RELEVANT FEATURES FOR MODELING\n",
      "\n",
      "✓ Selected 27 features\n",
      "  Features included: launch_speed, launch_angle, hit_distance_sc, bb_type, attack_angle, attack_direction, hc_x, hc_y, release_speed, pitch_type, plate_x, plate_z, arm_angle, release_spin_rate, spin_axis, balls, strikes, outs_when_up, inning, stand, p_throws, spray_angle, horizontal_distance, pitch_distance_from_center, count, runners_on, launch_speed_x_angle\n",
      "\n",
      "Categorical features (5): ['bb_type', 'pitch_type', 'stand', 'p_throws', 'count']\n",
      "Numeric features (22): ['launch_speed', 'launch_angle', 'hit_distance_sc', 'attack_angle', 'attack_direction', 'hc_x', 'hc_y', 'release_speed', 'plate_x', 'plate_z', 'arm_angle', 'release_spin_rate', 'spin_axis', 'balls', 'strikes', 'outs_when_up', 'inning', 'spray_angle', 'horizontal_distance', 'pitch_distance_from_center', 'runners_on', 'launch_speed_x_angle']\n",
      "\n",
      "Target (outcome_class) shape: (537681,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nSECTION 5: SELECTING RELEVANT FEATURES FOR MODELING\")\n",
    "\n",
    "# Define the features you want to use (no 'events', no outcome columns here)\n",
    "selected_features = [\n",
    "    # Batted ball characteristics\n",
    "    \"launch_speed\",      # Exit velocity in mph\n",
    "    \"launch_angle\",      # Angle off the bat in degrees\n",
    "    \"hit_distance_sc\",   # Projected hit distance\n",
    "    \"bb_type\",           # Batted ball type (fly_ball, ground_ball, etc.)\n",
    "    'attack_angle',\n",
    "    'attack_direction',\n",
    "\n",
    "    # Hit location\n",
    "    \"hc_x\",              # Hit coordinate X\n",
    "    \"hc_y\",              # Hit coordinate Y\n",
    "\n",
    "    # Pitch characteristics\n",
    "    \"release_speed\",     # Pitch velocity\n",
    "    \"pitch_type\",        # Type of pitch (FF, SL, CH, etc.)\n",
    "    \"plate_x\",           # Horizontal pitch location\n",
    "    \"plate_z\",           # Vertical pitch location\n",
    "    'arm_angle',\n",
    "    'release_spin_rate',\n",
    "    'spin_axis',\n",
    "\n",
    "    # Game situation\n",
    "    \"balls\",             # Ball count\n",
    "    \"strikes\",           # Strike count\n",
    "    \"outs_when_up\",      # Number of outs\n",
    "    \"inning\",            # Inning number\n",
    "\n",
    "    # Matchup information\n",
    "    \"stand\",             # Batter stance (L/R)\n",
    "    \"p_throws\",          # Pitcher handedness (L/R)\n",
    "\n",
    "\n",
    "    # Engineered features\n",
    "    \"spray_angle\",\n",
    "    \"horizontal_distance\",\n",
    "    \"pitch_distance_from_center\",\n",
    "    \"count\",\n",
    "    \"runners_on\",\n",
    "    \"launch_speed_x_angle\",\n",
    "]\n",
    "\n",
    "# Keep only those features that actually exist in `data`\n",
    "available_features = [col for col in selected_features if col in data.columns]\n",
    "X = data[available_features].copy()\n",
    "y = data[\"outcome_class\"].values.astype(np.int64)\n",
    "\n",
    "print(f\"\\n✓ Selected {len(available_features)} features\")\n",
    "print(f\"  Features included: {', '.join(available_features)}\")\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numerical_features   = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"Numeric features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"\\nTarget (outcome_class) shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "024a3d8a-2c2a-42bc-8bf7-c46724fd8886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SECTION 6: TRAIN/VAL/TEST SPLIT\n",
      "Split sizes:\n",
      "  X_train: (376362, 27)\n",
      "  X_val:   (80666, 27)\n",
      "  X_test:  (80653, 27)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSECTION 6: TRAIN/VAL/TEST SPLIT\")\n",
    "\n",
    "\n",
    "\n",
    "# First: train+val vs test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.15,      # 15% test\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Then: split temp into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.1765,    # 0.1765 * 0.85 ≈ 0.15 → 70/15/15 overall\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Split sizes:\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  X_val:  \", X_val.shape)\n",
    "print(\"  X_test: \", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "979b83dc-4a27-4269-9945-f609b9ead806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SECTION 7: BUILDING PREPROCESSING PIPELINE (IMPUTE + SCALE/OHE)\n",
      "\n",
      "Preprocessed shapes:\n",
      "  X_train_proc: (376362, 60)\n",
      "  X_val_proc:   (80666, 60)\n",
      "  X_test_proc:  (80653, 60)\n",
      "\n",
      "Model input dimension: 60\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSECTION 7: BUILDING PREPROCESSING PIPELINE (IMPUTE + SCALE/OHE)\")\n",
    "\n",
    "categorical_features = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric_features     = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # or \"constant\", fill_value=\"MISSING\"\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_val_proc   = preprocessor.transform(X_val)\n",
    "X_test_proc  = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"\\nPreprocessed shapes:\")\n",
    "print(\"  X_train_proc:\", X_train_proc.shape)\n",
    "print(\"  X_val_proc:  \", X_val_proc.shape)\n",
    "print(\"  X_test_proc: \", X_test_proc.shape)\n",
    "\n",
    "input_dim = X_train_proc.shape[1]\n",
    "print(f\"\\nModel input dimension: {input_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae0af892-3eef-4e52-8150-cca8d14539cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-13 11:20:16,430] A new study created in memory with name: no-name-b59c0669-4653-464a-88b5-f9051a7b359b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SECTION 8: HYPERPARAMETER SEARCH WITH OPTUNA\n",
      "Using device: cpu\n",
      "\n",
      "Running Optuna hyperparameter search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-13 11:20:38,715] Trial 0 finished with value: 0.5456559567360537 and parameters: {'hidden1': 286, 'hidden2': 259, 'hidden3': 134, 'dropout': 0.14656218940533366, 'lr': 3.087764792984312e-05, 'weight_decay': 1.0765871249202357e-05}. Best is trial 0 with value: 0.5456559567360537.\n",
      "[I 2026-01-13 11:21:04,587] Trial 1 finished with value: 0.5052766220656038 and parameters: {'hidden1': 428, 'hidden2': 415, 'hidden3': 49, 'dropout': 0.14271776347500642, 'lr': 1.069695115282547e-05, 'weight_decay': 6.18247014184058e-05}. Best is trial 0 with value: 0.5456559567360537.\n",
      "[I 2026-01-13 11:21:26,089] Trial 2 finished with value: 0.6339862328649466 and parameters: {'hidden1': 265, 'hidden2': 338, 'hidden3': 64, 'dropout': 0.3531781878078076, 'lr': 0.000933212824895327, 'weight_decay': 6.659613634825914e-05}. Best is trial 2 with value: 0.6339862328649466.\n",
      "[I 2026-01-13 11:21:44,176] Trial 3 finished with value: 0.500228607507455 and parameters: {'hidden1': 134, 'hidden2': 211, 'hidden3': 123, 'dropout': 0.32186364286313757, 'lr': 2.5587800755009342e-05, 'weight_decay': 1.1986440981347106e-06}. Best is trial 2 with value: 0.6339862328649466.\n",
      "[I 2026-01-13 11:22:12,435] Trial 4 finished with value: 0.6333962198245965 and parameters: {'hidden1': 354, 'hidden2': 421, 'hidden3': 125, 'dropout': 0.018857494482155013, 'lr': 0.0009364222417165691, 'weight_decay': 0.00011569399361362555}. Best is trial 2 with value: 0.6339862328649466.\n",
      "[I 2026-01-13 11:22:34,613] Trial 5 finished with value: 0.5817837302251175 and parameters: {'hidden1': 376, 'hidden2': 143, 'hidden3': 187, 'dropout': 0.2611254954601726, 'lr': 0.00013072771367498326, 'weight_decay': 6.6107810540490944e-06}. Best is trial 2 with value: 0.6339862328649466.\n",
      "[I 2026-01-13 11:22:51,713] Trial 6 finished with value: 0.543162266133119 and parameters: {'hidden1': 147, 'hidden2': 104, 'hidden3': 202, 'dropout': 0.4181679770455777, 'lr': 0.00010856696774964064, 'weight_decay': 3.164918260631072e-05}. Best is trial 2 with value: 0.6339862328649466.\n",
      "[I 2026-01-13 11:23:15,556] Trial 7 finished with value: 0.5989095088231514 and parameters: {'hidden1': 384, 'hidden2': 198, 'hidden3': 252, 'dropout': 0.44195506950581803, 'lr': 0.00031714202004908627, 'weight_decay': 1.044551305067538e-05}. Best is trial 2 with value: 0.6339862328649466.\n",
      "[I 2026-01-13 11:23:44,216] Trial 8 finished with value: 0.6140979627322141 and parameters: {'hidden1': 470, 'hidden2': 494, 'hidden3': 32, 'dropout': 0.17035668528105635, 'lr': 0.0009241859362314502, 'weight_decay': 0.0002640336127477709}. Best is trial 2 with value: 0.6339862328649466.\n",
      "[I 2026-01-13 11:24:05,189] Trial 9 finished with value: 0.556784744539367 and parameters: {'hidden1': 339, 'hidden2': 249, 'hidden3': 57, 'dropout': 0.320548220782078, 'lr': 7.010641189339496e-05, 'weight_decay': 1.9998184886386083e-06}. Best is trial 2 with value: 0.6339862328649466.\n",
      "[I 2026-01-13 11:24:26,710] Trial 10 finished with value: 0.5873111857435579 and parameters: {'hidden1': 242, 'hidden2': 326, 'hidden3': 86, 'dropout': 0.49584058824699034, 'lr': 0.00031868758694786505, 'weight_decay': 0.0006105584408163028}. Best is trial 2 with value: 0.6339862328649466.\n",
      "[I 2026-01-13 11:24:49,636] Trial 11 finished with value: 0.6314337682434205 and parameters: {'hidden1': 233, 'hidden2': 362, 'hidden3': 95, 'dropout': 0.020143020629008995, 'lr': 0.0009137765018985162, 'weight_decay': 0.00011784130285887428}. Best is trial 2 with value: 0.6339862328649466.\n",
      "[I 2026-01-13 11:25:16,075] Trial 12 finished with value: 0.6224991180960375 and parameters: {'hidden1': 282, 'hidden2': 449, 'hidden3': 168, 'dropout': 0.0030660680288675767, 'lr': 0.0004237538681928751, 'weight_decay': 0.00016130000735957103}. Best is trial 2 with value: 0.6339862328649466.\n",
      "[I 2026-01-13 11:25:37,913] Trial 13 finished with value: 0.6136656578261113 and parameters: {'hidden1': 200, 'hidden2': 355, 'hidden3': 92, 'dropout': 0.36344943334942037, 'lr': 0.0004893673275878119, 'weight_decay': 0.0005222934058319314}. Best is trial 2 with value: 0.6339862328649466.\n",
      "[I 2026-01-13 11:26:03,638] Trial 14 finished with value: 0.5940388148402811 and parameters: {'hidden1': 333, 'hidden2': 416, 'hidden3': 120, 'dropout': 0.2285719253561122, 'lr': 0.0002086642984347431, 'weight_decay': 3.996233798024454e-05}. Best is trial 2 with value: 0.6339862328649466.\n",
      "[I 2026-01-13 11:26:36,410] Trial 15 finished with value: 0.6248399304984869 and parameters: {'hidden1': 510, 'hidden2': 496, 'hidden3': 158, 'dropout': 0.07671844245809462, 'lr': 0.0009960659480875134, 'weight_decay': 9.361827264280336e-05}. Best is trial 2 with value: 0.6339862328649466.\n",
      "[I 2026-01-13 11:27:00,784] Trial 16 finished with value: 0.6291664751925337 and parameters: {'hidden1': 401, 'hidden2': 308, 'hidden3': 71, 'dropout': 0.24346137225687117, 'lr': 0.0005667344073331738, 'weight_decay': 0.0003083213794889901}. Best is trial 2 with value: 0.6339862328649466.\n",
      "[I 2026-01-13 11:27:25,683] Trial 17 finished with value: 0.5970944811732595 and parameters: {'hidden1': 299, 'hidden2': 389, 'hidden3': 111, 'dropout': 0.068078068923772, 'lr': 0.00017999778835167747, 'weight_decay': 1.3051989589254073e-05}. Best is trial 2 with value: 0.6339862328649466.\n",
      "[I 2026-01-13 11:27:50,691] Trial 18 finished with value: 0.5411450305618762 and parameters: {'hidden1': 192, 'hidden2': 452, 'hidden3': 222, 'dropout': 0.38358708652782864, 'lr': 5.9294137680305306e-05, 'weight_decay': 2.4229223398120476e-05}. Best is trial 2 with value: 0.6339862328649466.\n",
      "[I 2026-01-13 11:28:13,799] Trial 19 finished with value: 0.6242624734688675 and parameters: {'hidden1': 326, 'hidden2': 291, 'hidden3': 151, 'dropout': 0.2969013136387886, 'lr': 0.0006012070717183417, 'weight_decay': 3.924320835273561e-06}. Best is trial 2 with value: 0.6339862328649466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best trial found by Optuna:\n",
      "  Value (val macro-F1): 0.6339862328649466\n",
      "  Params: {'hidden1': 265, 'hidden2': 338, 'hidden3': 64, 'dropout': 0.3531781878078076, 'lr': 0.000933212824895327, 'weight_decay': 6.659613634825914e-05}\n",
      "\n",
      "Training final model with best hyperparameters...\n",
      "Epoch 001 | train_loss=0.9874 | val_acc=0.7711 | val_macro_f1=0.5582\n",
      "Epoch 002 | train_loss=0.8252 | val_acc=0.7936 | val_macro_f1=0.5838\n",
      "Epoch 003 | train_loss=0.7823 | val_acc=0.8187 | val_macro_f1=0.6070\n",
      "Epoch 004 | train_loss=0.7562 | val_acc=0.8110 | val_macro_f1=0.6035\n",
      "Epoch 005 | train_loss=0.7340 | val_acc=0.8130 | val_macro_f1=0.6096\n",
      "Epoch 006 | train_loss=0.7193 | val_acc=0.8197 | val_macro_f1=0.6160\n",
      "Epoch 007 | train_loss=0.7097 | val_acc=0.8142 | val_macro_f1=0.6057\n",
      "Epoch 008 | train_loss=0.6993 | val_acc=0.8165 | val_macro_f1=0.6159\n",
      "Epoch 009 | train_loss=0.6903 | val_acc=0.8113 | val_macro_f1=0.6175\n",
      "Epoch 010 | train_loss=0.6871 | val_acc=0.8260 | val_macro_f1=0.6255\n",
      "Epoch 011 | train_loss=0.6807 | val_acc=0.8201 | val_macro_f1=0.6224\n",
      "Epoch 012 | train_loss=0.6734 | val_acc=0.8049 | val_macro_f1=0.6111\n",
      "Epoch 013 | train_loss=0.6686 | val_acc=0.8233 | val_macro_f1=0.6232\n",
      "Epoch 014 | train_loss=0.6606 | val_acc=0.8283 | val_macro_f1=0.6265\n",
      "Epoch 015 | train_loss=0.6578 | val_acc=0.8208 | val_macro_f1=0.6241\n",
      "Epoch 016 | train_loss=0.6554 | val_acc=0.8329 | val_macro_f1=0.6318\n",
      "Epoch 017 | train_loss=0.6466 | val_acc=0.8238 | val_macro_f1=0.6240\n",
      "Epoch 018 | train_loss=0.6434 | val_acc=0.8271 | val_macro_f1=0.6249\n",
      "Epoch 019 | train_loss=0.6421 | val_acc=0.8408 | val_macro_f1=0.6415\n",
      "Epoch 020 | train_loss=0.6375 | val_acc=0.8228 | val_macro_f1=0.6232\n",
      "Epoch 021 | train_loss=0.6391 | val_acc=0.8404 | val_macro_f1=0.6322\n",
      "Epoch 022 | train_loss=0.6313 | val_acc=0.8418 | val_macro_f1=0.6357\n",
      "Epoch 023 | train_loss=0.6321 | val_acc=0.8333 | val_macro_f1=0.6288\n",
      "Epoch 024 | train_loss=0.6234 | val_acc=0.8339 | val_macro_f1=0.6293\n",
      "Epoch 025 | train_loss=0.6209 | val_acc=0.8317 | val_macro_f1=0.6284\n",
      "Early stopping triggered.\n",
      "\n",
      "Loaded best model from epoch 19 with val_macro_f1=0.6415\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSECTION 8: HYPERPARAMETER SEARCH WITH OPTUNA\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# ----- Tensors & Dataloaders -----\n",
    "\n",
    "X_train_t = torch.tensor(X_train_proc, dtype=torch.float32)\n",
    "X_val_t   = torch.tensor(X_val_proc,   dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test_proc,  dtype=torch.float32)\n",
    "\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val_t   = torch.tensor(y_val,   dtype=torch.long)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train_t, y_train_t),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(X_val_t, y_val_t),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(X_test_t, y_test_t),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# ----- Model definition -----\n",
    "\n",
    "class MLP5(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dims=None, dropout: float = 0.2, num_classes: int = 5):\n",
    "        super().__init__()\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [256, 128, 64]\n",
    "\n",
    "        layers = []\n",
    "        prev = in_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev = h\n",
    "\n",
    "        layers.append(nn.Linear(prev, num_classes))  # logits\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ----- Evaluation helper -----\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "def evaluate_multiclass(loader, mdl):\n",
    "    mdl.eval()\n",
    "    all_y = []\n",
    "    all_pred = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            logits = mdl(xb)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_y.append(yb.cpu().numpy())\n",
    "            all_pred.append(preds.cpu().numpy())\n",
    "    y_true = np.concatenate(all_y)\n",
    "    y_pred = np.concatenate(all_pred)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    return {\"acc\": acc, \"macro_f1\": macro_f1, \"y_true\": y_true, \"y_pred\": y_pred}\n",
    "\n",
    "\n",
    "print(\"\\nRunning Optuna hyperparameter search...\")\n",
    "\n",
    "def objective(trial):\n",
    "    # ---- Hyperparameters to search ----\n",
    "    hidden1 = trial.suggest_int(\"hidden1\", 128, 512)\n",
    "    hidden2 = trial.suggest_int(\"hidden2\", 64, 512)\n",
    "    hidden3 = trial.suggest_int(\"hidden3\", 32, 256)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "\n",
    "    # ---- Build model for this trial ----\n",
    "    model = MLP5(\n",
    "        in_dim=X_train_proc.shape[1],\n",
    "        hidden_dims=[hidden1, hidden2, hidden3],\n",
    "        dropout=dropout,\n",
    "        num_classes=5\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Class weights for imbalance (computed from train labels)\n",
    "    class_counts = np.bincount(y_train)\n",
    "    num_classes_ = len(class_counts)\n",
    "    class_weights = (class_counts.sum() / (num_classes_ * class_counts)).astype(np.float32)\n",
    "    class_weights_t = torch.tensor(class_weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_t)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    EPOCHS_SEARCH = 8  # small number of epochs per trial to keep search feasible\n",
    "\n",
    "    for epoch in range(EPOCHS_SEARCH):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    metrics = evaluate_multiclass(val_loader, model)\n",
    "    val_macro_f1 = metrics[\"macro_f1\"]\n",
    "\n",
    "    # We want to maximize macro-F1\n",
    "    return val_macro_f1\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)  # bump this on HPC (e.g., 50–200)\n",
    "\n",
    "print(\"\\nBest trial found by Optuna:\")\n",
    "print(\"  Value (val macro-F1):\", study.best_trial.value)\n",
    "print(\"  Params:\", study.best_trial.params)\n",
    "\n",
    "best_params = study.best_params\n",
    "\n",
    "print(\"\\nTraining final model with best hyperparameters...\")\n",
    "\n",
    "model = MLP5(\n",
    "    in_dim=X_train_proc.shape[1],\n",
    "    hidden_dims=[\n",
    "        best_params[\"hidden1\"],\n",
    "        best_params[\"hidden2\"],\n",
    "        best_params[\"hidden3\"],\n",
    "    ],\n",
    "    dropout=best_params[\"dropout\"],\n",
    "    num_classes=5,\n",
    ").to(DEVICE)\n",
    "\n",
    "# Recompute class weights for final training\n",
    "class_counts = np.bincount(y_train)\n",
    "num_classes_ = len(class_counts)\n",
    "class_weights = (class_counts.sum() / (num_classes_ * class_counts)).astype(np.float32)\n",
    "class_weights_t = torch.tensor(class_weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_t)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=best_params[\"lr\"],\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "EPOCHS = 40\n",
    "PATIENCE = 6\n",
    "best_val_f1 = -np.inf\n",
    "best_state = None\n",
    "pat = PATIENCE\n",
    "\n",
    "history_epochs = []\n",
    "history_train_loss = []\n",
    "history_val_acc = []\n",
    "history_val_macro_f1 = []\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    train_loss = running_loss / max(n_batches, 1)\n",
    "    val_metrics = evaluate_multiclass(val_loader, model)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:03d} | \"\n",
    "        f\"train_loss={train_loss:.4f} | \"\n",
    "        f\"val_acc={val_metrics['acc']:.4f} | \"\n",
    "        f\"val_macro_f1={val_metrics['macro_f1']:.4f}\"\n",
    "    )\n",
    "\n",
    "    history_epochs.append(epoch)\n",
    "    history_train_loss.append(train_loss)\n",
    "    history_val_acc.append(val_metrics[\"acc\"])\n",
    "    history_val_macro_f1.append(val_metrics[\"macro_f1\"])\n",
    "\n",
    "    # Early stopping on validation macro-F1\n",
    "    if val_metrics[\"macro_f1\"] > best_val_f1:\n",
    "        best_val_f1 = val_metrics[\"macro_f1\"]\n",
    "        best_state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "        }\n",
    "        pat = PATIENCE\n",
    "    else:\n",
    "        pat -= 1\n",
    "        if pat <= 0:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best model state\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state[\"model_state\"])\n",
    "    print(f\"\\nLoaded best model from epoch {best_state['epoch']} with val_macro_f1={best_val_f1:.4f}\")\n",
    "else:\n",
    "    print(\"\\nWarning: best_state is None; using last epoch model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4d7387c-128f-42cf-b611-2f44fc7baa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SECTION 9: FINAL TRAINING WITH BEST HYPERPARAMETERS\n",
      "\n",
      "Training final model with best hyperparameters...\n",
      "Epoch 001 | train_loss=0.9820 | val_acc=0.7621 | val_macro_f1=0.5589\n",
      "Epoch 002 | train_loss=0.8313 | val_acc=0.7686 | val_macro_f1=0.5616\n",
      "Epoch 003 | train_loss=0.7847 | val_acc=0.8101 | val_macro_f1=0.6036\n",
      "Epoch 004 | train_loss=0.7568 | val_acc=0.8069 | val_macro_f1=0.6007\n",
      "Epoch 005 | train_loss=0.7360 | val_acc=0.8285 | val_macro_f1=0.6245\n",
      "Epoch 006 | train_loss=0.7222 | val_acc=0.8146 | val_macro_f1=0.6058\n",
      "Epoch 007 | train_loss=0.7096 | val_acc=0.8319 | val_macro_f1=0.6213\n",
      "Epoch 008 | train_loss=0.6990 | val_acc=0.8241 | val_macro_f1=0.6248\n",
      "Epoch 009 | train_loss=0.6934 | val_acc=0.8133 | val_macro_f1=0.6068\n",
      "Epoch 010 | train_loss=0.6836 | val_acc=0.8306 | val_macro_f1=0.6230\n",
      "Epoch 011 | train_loss=0.6771 | val_acc=0.8224 | val_macro_f1=0.6196\n",
      "Epoch 012 | train_loss=0.6703 | val_acc=0.8228 | val_macro_f1=0.6195\n",
      "Epoch 013 | train_loss=0.6665 | val_acc=0.8177 | val_macro_f1=0.6223\n",
      "Epoch 014 | train_loss=0.6601 | val_acc=0.8400 | val_macro_f1=0.6371\n",
      "Epoch 015 | train_loss=0.6570 | val_acc=0.8289 | val_macro_f1=0.6284\n",
      "Epoch 016 | train_loss=0.6515 | val_acc=0.8323 | val_macro_f1=0.6350\n",
      "Epoch 017 | train_loss=0.6481 | val_acc=0.8301 | val_macro_f1=0.6294\n",
      "Epoch 018 | train_loss=0.6468 | val_acc=0.8357 | val_macro_f1=0.6299\n",
      "Epoch 019 | train_loss=0.6401 | val_acc=0.8359 | val_macro_f1=0.6369\n",
      "Epoch 020 | train_loss=0.6388 | val_acc=0.8283 | val_macro_f1=0.6295\n",
      "Early stopping triggered.\n",
      "\n",
      "Loaded best model from epoch 14 with val_macro_f1=0.6371\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSECTION 9: FINAL TRAINING WITH BEST HYPERPARAMETERS\")\n",
    "\n",
    "print(\"\\nTraining final model with best hyperparameters...\")\n",
    "\n",
    "model = MLP5(\n",
    "    in_dim=X_train_proc.shape[1],\n",
    "    hidden_dims=[\n",
    "        best_params[\"hidden1\"],\n",
    "        best_params[\"hidden2\"],\n",
    "        best_params[\"hidden3\"],\n",
    "    ],\n",
    "    dropout=best_params[\"dropout\"],\n",
    "    num_classes=5,  # or n_classes\n",
    ").to(DEVICE)\n",
    "\n",
    "# Recompute class weights for final training\n",
    "class_counts = np.bincount(y_train)\n",
    "num_classes_ = len(class_counts)\n",
    "class_weights = (class_counts.sum() / (num_classes_ * class_counts)).astype(np.float32)\n",
    "class_weights_t = torch.tensor(class_weights, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_t)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=best_params[\"lr\"],\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "EPOCHS = 40\n",
    "PATIENCE = 6\n",
    "best_val_f1 = -np.inf\n",
    "best_state = None\n",
    "pat = PATIENCE\n",
    "\n",
    "history_epochs = []\n",
    "history_train_loss = []\n",
    "history_val_acc = []\n",
    "history_val_macro_f1 = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    train_loss = running_loss / max(n_batches, 1)\n",
    "\n",
    "    # Validation metrics\n",
    "    val_metrics = evaluate_multiclass(val_loader, model)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:03d} | \"\n",
    "        f\"train_loss={train_loss:.4f} | \"\n",
    "        f\"val_acc={val_metrics['acc']:.4f} | \"\n",
    "        f\"val_macro_f1={val_metrics['macro_f1']:.4f}\"\n",
    "    )\n",
    "\n",
    "    history_epochs.append(epoch)\n",
    "    history_train_loss.append(train_loss)\n",
    "    history_val_acc.append(val_metrics[\"acc\"])\n",
    "    history_val_macro_f1.append(val_metrics[\"macro_f1\"])\n",
    "\n",
    "    # Early stopping on validation macro-F1\n",
    "    if val_metrics[\"macro_f1\"] > best_val_f1:\n",
    "        best_val_f1 = val_metrics[\"macro_f1\"]\n",
    "        best_state = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "        }\n",
    "        pat = PATIENCE\n",
    "    else:\n",
    "        pat -= 1\n",
    "        if pat <= 0:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best model state\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state[\"model_state\"])\n",
    "    print(f\"\\nLoaded best model from epoch {best_state['epoch']} with val_macro_f1={best_val_f1:.4f}\")\n",
    "else:\n",
    "    print(\"\\nWarning: best_state is None; using last epoch model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97323d44-f89e-4bf1-be1e-c438261dfe90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SECTION 10: FINAL EVALUATION\n",
      "Train -> Acc: 0.832, Macro F1: 0.643\n",
      "Val -> Acc: 0.828, Macro F1: 0.629\n",
      "Test -> Acc: 0.827, Macro F1: 0.630\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSECTION 10: FINAL EVALUATION\")\n",
    "\n",
    "def evaluate_split(name, loader, model):\n",
    "    metrics = evaluate_multiclass(loader, model)\n",
    "    acc = metrics[\"acc\"]\n",
    "    f1 = metrics[\"macro_f1\"]\n",
    "    preds = metrics[\"y_pred\"]\n",
    "    labels = metrics[\"y_true\"]\n",
    "    print(f\"{name} -> Acc: {acc:.3f}, Macro F1: {f1:.3f}\")\n",
    "    return acc, f1, preds, labels\n",
    "\n",
    "train_metrics = evaluate_split(\"Train\", train_loader, model)\n",
    "val_metrics   = evaluate_split(\"Val\",   val_loader,   model)\n",
    "test_metrics  = evaluate_split(\"Test\",  test_loader,  model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "535778f0-f995-41ba-a499-f996c4477370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SECTION 11: VISUALIZATIONS\n",
      "Saved plots:\n",
      "  training_loss.png\n",
      "  validation_metrics.png\n",
      "  confusion_matrix_test.png\n",
      "\n",
      "All done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nSECTION 11: VISUALIZATIONS\")\n",
    "\n",
    "# 8.1 Loss curve\n",
    "plt.figure()\n",
    "plt.plot(history_epochs, history_train_loss, label=\"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_loss.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# 8.2 Validation metrics curves\n",
    "plt.figure()\n",
    "plt.plot(history_epochs, history_val_acc, label=\"Val Acc\")\n",
    "plt.plot(history_epochs, history_val_macro_f1, label=\"Val Macro F1\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Validation Accuracy & Macro F1\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"validation_metrics.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# 8.3 Confusion matrix on test\n",
    "_, _, test_preds, test_labels = test_metrics\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "plt.figure()\n",
    "disp.plot(values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix - Test Set\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix_test.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved plots:\")\n",
    "print(\"  training_loss.png\")\n",
    "print(\"  validation_metrics.png\")\n",
    "print(\"  confusion_matrix_test.png\")\n",
    "\n",
    "print(\"\\nAll done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcb6f1-975c-4a7f-9259-5457bb2c8921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
